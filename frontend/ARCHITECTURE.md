# System Architecture & Data Flow

## ğŸ—ï¸ High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         WEB BROWSER (User Interface)                 â”‚
â”‚                                                                       â”‚
â”‚  [Dashboard] [Upload Schema] [Upload Script] [Evaluate] [Results]   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚ HTTP Requests
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FLASK WEB SERVER (Port 5000)                      â”‚
â”‚                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Routes & Controllers (app.py)                                â”‚   â”‚
â”‚  â”‚                                                               â”‚   â”‚
â”‚  â”‚ â€¢ Dashboard Routes          â€¢ Results Routes                â”‚   â”‚
â”‚  â”‚ â€¢ Schema Upload Routes      â€¢ Manual Evaluation Routes       â”‚   â”‚
â”‚  â”‚ â€¢ Script Upload Routes      â€¢ PDF Viewer Routes             â”‚   â”‚
â”‚  â”‚ â€¢ Evaluation Routes         â€¢ API Endpoints                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Helper Functions                                              â”‚   â”‚
â”‚  â”‚ â€¢ CLI Command Execution     â€¢ ObjectId Conversion            â”‚   â”‚
â”‚  â”‚ â€¢ MongoDB Operations        â€¢ Result Formatting              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚              â”‚                     â”‚                      â”‚
     â”‚ CLI Calls    â”‚ DB Queries         â”‚ File Uploads         â”‚ API Calls
     â–¼              â–¼                     â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CLI Scripts  â”‚ â”‚  MongoDB     â”‚ â”‚  File System â”‚ â”‚ External APIs    â”‚
â”‚              â”‚ â”‚              â”‚ â”‚              â”‚ â”‚                  â”‚
â”‚ â€¢ ocr_pdf    â”‚ â”‚ Databases:   â”‚ â”‚ â€¢ Uploads/   â”‚ â”‚ â€¢ OpenAI         â”‚
â”‚ â€¢ scheme_    â”‚ â”‚              â”‚ â”‚ â€¢ PDFs       â”‚ â”‚ â€¢ Google Gemini  â”‚
â”‚   extractor  â”‚ â”‚ â€¢ ai_eval... â”‚ â”‚              â”‚ â”‚                  â”‚
â”‚ â€¢ comparator â”‚ â”‚ â€¢ schema_db  â”‚ â”‚ static/      â”‚ â”‚                  â”‚
â”‚              â”‚ â”‚ â€¢ result_db  â”‚ â”‚ uploads/     â”‚ â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Database Collections Diagram

```
MongoDB Cluster
â”‚
â”œâ”€ ai_evaluation_system (Database)
â”‚  â”‚
â”‚  â””â”€ ocr_extracted_answers (Collection)
â”‚     â”œâ”€ _id: ObjectId
â”‚     â”œâ”€ examId: ObjectId â†’ references schema_db
â”‚     â”œâ”€ studentId: ObjectId
â”‚     â”œâ”€ pageNumber: Int
â”‚     â”œâ”€ questionNumber: Int
â”‚     â”œâ”€ rawText: String (OCR output)
â”‚     â”œâ”€ confidence: Float (0-1)
â”‚     â”œâ”€ fileName: String
â”‚     â””â”€ createdAt: ISODate
â”‚
â”œâ”€ schema_db (Database)
â”‚  â”‚
â”‚  â””â”€ schema_extracted_answers (Collection)
â”‚     â”œâ”€ _id: ObjectId
â”‚     â”œâ”€ examId: ObjectId
â”‚     â”œâ”€ professorId: ObjectId
â”‚     â”œâ”€ subjectId: ObjectId
â”‚     â”œâ”€ rawExtractedText: String
â”‚     â”œâ”€ structuredData: Document
â”‚     â”‚  â””â”€ questions: Array of Objects
â”‚     â”‚     â”œâ”€ questionNumber: Int
â”‚     â”‚     â”œâ”€ questionText: String
â”‚     â”‚     â”œâ”€ maxMarks: Int
â”‚     â”‚     â”œâ”€ concepts: Array
â”‚     â”‚     â””â”€ evaluationCriteria: Object
â”‚     â””â”€ createdAt: ISODate
â”‚
â””â”€ result_db (Database)
   â”‚
   â”œâ”€ evaluations (Collection)
   â”‚  â”œâ”€ _id: ObjectId
   â”‚  â”œâ”€ examId: ObjectId â†’ references schema_db
   â”‚  â”œâ”€ studentId: ObjectId
   â”‚  â”œâ”€ perQuestion: Array of Objects
   â”‚  â”‚  â”œâ”€ questionNumber: Int
   â”‚  â”‚  â”œâ”€ scoredMarks: Int
   â”‚  â”‚  â”œâ”€ maxMarks: Int
   â”‚  â”‚  â”œâ”€ similarity: Float
   â”‚  â”‚  â”œâ”€ ocrConfidenceAvg: Float
   â”‚  â”‚  â””â”€ flags: Array
   â”‚  â”œâ”€ overall: Object
   â”‚  â”‚  â”œâ”€ totalScoredMarks: Int
   â”‚  â”‚  â”œâ”€ totalMaxMarks: Int
   â”‚  â”‚  â””â”€ percentage: Float
   â”‚  â””â”€ generatedAt: ISODate
   â”‚
   â””â”€ manual_evaluations (Collection)
      â”œâ”€ _id: ObjectId
      â”œâ”€ examId: ObjectId
      â”œâ”€ studentId: ObjectId
      â”œâ”€ manualMarks: Object
      â”œâ”€ notes: String
      â””â”€ evaluatedAt: ISODate
```

## ğŸ”„ Complete Data Flow

```
START: User Action
â”‚
â”œâ”€ [1] UPLOAD SCHEMA
â”‚  â”‚
â”‚  â”œâ”€ User selects PDF from disk
â”‚  â”œâ”€ Flask validates: PDF format, size < 100MB
â”‚  â”œâ”€ Save to: frontend/static/uploads/TIMESTAMP_filename.pdf
â”‚  â”‚
â”‚  â”œâ”€ Execute: scheme_extractor.py <pdf_path> --exam-id <id> ...
â”‚  â”‚  â”‚
â”‚  â”‚  â”œâ”€ PyMuPDF opens PDF
â”‚  â”‚  â”œâ”€ Extract text (direct or OCR if image)
â”‚  â”‚  â”‚  â””â”€ If image: Send to OpenAI Vision â†’ OCR text
â”‚  â”‚  â”‚
â”‚  â”‚  â”œâ”€ Send raw text to GPT-4 + SCHEME_EXTRACTION_PROMPT
â”‚  â”‚  â”œâ”€ Parse JSON response
â”‚  â”‚  â”‚  â””â”€ Extract: Questions, Max Marks, Concepts, Criteria
â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€ Store in schema_db.schema_extracted_answers
â”‚  â”‚
â”‚  â””â”€ Return: Exam ID to user
â”‚
â”œâ”€ [2] UPLOAD SCRIPT
â”‚  â”‚
â”‚  â”œâ”€ User provides: Exam ID (from step 1), Student ID
â”‚  â”œâ”€ User selects answer script PDF
â”‚  â”œâ”€ Flask validates input
â”‚  â”‚
â”‚  â”œâ”€ Execute: ocr_pdf.py <pdf_path> --exam-id <id> --student-id <id>
â”‚  â”‚  â”‚
â”‚  â”‚  â”œâ”€ For each page in PDF:
â”‚  â”‚  â”‚  â”œâ”€ Convert page to image (200 DPI)
â”‚  â”‚  â”‚  â”œâ”€ Send to OpenAI Vision API
â”‚  â”‚  â”‚  â”‚  â””â”€ OCR prompt + base64 image
â”‚  â”‚  â”‚  â”œâ”€ Extract: Text + Confidence + Question Number
â”‚  â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â””â”€ Store record in ai_evaluation_system.ocr_extracted_answers
â”‚  â”‚  â”‚     {
â”‚  â”‚  â”‚       examId, studentId, pageNumber, questionNumber,
â”‚  â”‚  â”‚       rawText, confidence, fileName, createdAt
â”‚  â”‚  â”‚     }
â”‚  â”‚
â”‚  â””â”€ Return: Success + Page count
â”‚
â”œâ”€ [3] RUN EVALUATION
â”‚  â”‚
â”‚  â”œâ”€ User provides: Exam ID, Student ID
â”‚  â”œâ”€ Flask validates IDs exist
â”‚  â”‚
â”‚  â”œâ”€ Execute: comparator.py --exam-id <id> --student-id <id>
â”‚  â”‚  â”‚
â”‚  â”‚  â”œâ”€ [3a] Load Data
â”‚  â”‚  â”‚  â”œâ”€ Query schema_db for schema â†’ get structured questions
â”‚  â”‚  â”‚  â””â”€ Query ai_evaluation_system for OCR â†’ get student text
â”‚  â”‚  â”‚
â”‚  â”‚  â”œâ”€ [3b] For Each Question:
â”‚  â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”œâ”€ Build reference text from scheme
â”‚  â”‚  â”‚  â”‚  â”œâ”€ Question text
â”‚  â”‚  â”‚  â”‚  â”œâ”€ Reference answer
â”‚  â”‚  â”‚  â”‚  â”œâ”€ Key concepts
â”‚  â”‚  â”‚  â”‚  â””â”€ Must-include points
â”‚  â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”œâ”€ Aggregate student answer text
â”‚  â”‚  â”‚  â”‚  â”œâ”€ Find pages with matching question number
â”‚  â”‚  â”‚  â”‚  â”œâ”€ Sort by page number
â”‚  â”‚  â”‚  â”‚  â””â”€ Concatenate text
â”‚  â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”œâ”€ Generate Embeddings
â”‚  â”‚  â”‚  â”‚  â”œâ”€ Send reference text â†’ OpenAI
â”‚  â”‚  â”‚  â”‚  â”‚  â””â”€ model: text-embedding-3-small
â”‚  â”‚  â”‚  â”‚  â”‚  â””â”€ Returns: 1536-d vector
â”‚  â”‚  â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â””â”€ Send student text â†’ OpenAI
â”‚  â”‚  â”‚  â”‚     â””â”€ Returns: 1536-d vector
â”‚  â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”œâ”€ Calculate Similarity
â”‚  â”‚  â”‚  â”‚  â”œâ”€ cosine(reference_vector, student_vector)
â”‚  â”‚  â”‚  â”‚  â””â”€ Returns: 0-1 score
â”‚  â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”œâ”€ Score Question
â”‚  â”‚  â”‚  â”‚  â”œâ”€ scored_marks = round(similarity Ã— max_marks)
â”‚  â”‚  â”‚  â”‚  â””â”€ clamp(scored_marks, 0, max_marks)
â”‚  â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”œâ”€ Flag Check
â”‚  â”‚  â”‚  â”‚  â”œâ”€ If similarity < 0.50: LOW_SIMILARITY
â”‚  â”‚  â”‚  â”‚  â”œâ”€ If 0.50 <= similarity < 0.65: BORDERLINE
â”‚  â”‚  â”‚  â”‚  â””â”€ If OCR confidence < 0.55: LOW_OCR_CONFIDENCE
â”‚  â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â””â”€ Gemini Verification
â”‚  â”‚  â”‚     â”œâ”€ Send verification prompt to Gemini
â”‚  â”‚  â”‚     â”œâ”€ Includes: student answer, reference, similarity
â”‚  â”‚  â”‚     â”œâ”€ Gemini returns: flag + reason + suggested_marks
â”‚  â”‚  â”‚     â””â”€ Add flag if verification returns true
â”‚  â”‚  â”‚
â”‚  â”‚  â”œâ”€ [3c] Calculate Overall Score
â”‚  â”‚  â”‚  â”œâ”€ total_scored = sum(scored_marks for all questions)
â”‚  â”‚  â”‚  â”œâ”€ total_max = sum(max_marks for all questions)
â”‚  â”‚  â”‚  â””â”€ percentage = (total_scored / total_max) Ã— 100
â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€ [3d] Store Result
â”‚  â”‚     â””â”€ Insert to result_db.evaluations:
â”‚  â”‚        {
â”‚  â”‚          examId, studentId, schemeRefId,
â”‚  â”‚          perQuestion: [ { qNum, max, scored, similarity, flags, verification } ],
â”‚  â”‚          overall: { totalMax, totalScored, percentage },
â”‚  â”‚          generatedAt
â”‚  â”‚        }
â”‚  â”‚
â”‚  â””â”€ Return: Result ID to user
â”‚
â”œâ”€ [4] VIEW RESULTS
â”‚  â”‚
â”‚  â”œâ”€ User navigates to Results page
â”‚  â”œâ”€ Flask queries result_db.evaluations
â”‚  â”œâ”€ Format and display:
â”‚  â”‚  â”œâ”€ Overall score circle with percentage
â”‚  â”‚  â”œâ”€ Per-question cards with metrics
â”‚  â”‚  â”œâ”€ Flagged items highlighted
â”‚  â”‚  â””â”€ Action buttons
â”‚  â”‚
â”‚  â””â”€ Display: Scores, similarity, confidence, flags
â”‚
â”œâ”€ [5] MANUAL EVALUATION (Optional)
â”‚  â”‚
â”‚  â”œâ”€ User clicks "Manual Evaluation" from results
â”‚  â”œâ”€ Page loads with auto-generated form
â”‚  â”‚  â””â”€ One input field per question (max to max_marks)
â”‚  â”‚
â”‚  â”œâ”€ User:
â”‚  â”‚  â”œâ”€ Enters manual marks for each question
â”‚  â”‚  â”œâ”€ Adds optional notes
â”‚  â”‚  â””â”€ Clicks Save
â”‚  â”‚
â”‚  â”œâ”€ Flask:
â”‚  â”‚  â”œâ”€ Validates marks (0 to max_marks)
â”‚  â”‚  â””â”€ Upsert to result_db.manual_evaluations
â”‚  â”‚
â”‚  â””â”€ Return: Success message
â”‚
â”œâ”€ [6] VIEW OCR TEXT
â”‚  â”‚
â”‚  â”œâ”€ User clicks "View Extracted Text"
â”‚  â”œâ”€ Flask queries ai_evaluation_system.ocr_extracted_answers
â”‚  â”‚  â””â”€ Filter by examId + studentId, sort by pageNumber
â”‚  â”‚
â”‚  â”œâ”€ Display:
â”‚  â”‚  â”œâ”€ Page list on left (with question numbers)
â”‚  â”‚  â”œâ”€ OCR text in center
â”‚  â”‚  â”œâ”€ Confidence scores and controls
â”‚  â”‚  â””â”€ Export buttons
â”‚  â”‚
â”‚  â”œâ”€ User interactions:
â”‚  â”‚  â”œâ”€ Click page â†’ update text view
â”‚  â”‚  â”œâ”€ Zoom In/Out â†’ adjust font size
â”‚  â”‚  â”œâ”€ Download â†’ save as .txt
â”‚  â”‚  â””â”€ Copy â†’ copy to clipboard
â”‚  â”‚
â”‚  â””â”€ All operations are client-side (JavaScript)
â”‚
â””â”€ END
```

## ğŸ”— Component Interaction Diagram

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Flask App     â”‚
                    â”‚    (app.py)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                    â”‚                    â”‚
        â–¼                    â–¼                    â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   MongoDB   â”‚   â”‚ CLI Scripts  â”‚  â”‚ External APIs    â”‚
   â”‚             â”‚   â”‚              â”‚  â”‚                  â”‚
   â”‚ 3 Databases â”‚   â”‚ â€¢ ocr_pdf    â”‚  â”‚ â€¢ OpenAI         â”‚
   â”‚ 5 Collections   â”‚ â€¢ scheme_ex  â”‚  â”‚   - Vision API   â”‚
   â”‚             â”‚   â”‚ â€¢ comparator â”‚  â”‚   - Embeddings   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚              â”‚  â”‚ â€¢ Google Gemini  â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                  â”‚
                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚         User Interface (Browser)             â”‚
        â”‚                                              â”‚
        â”‚  Templates + CSS + JavaScript                â”‚
        â”‚  â€¢ 9 HTML templates                          â”‚
        â”‚  â€¢ Responsive design                         â”‚
        â”‚  â€¢ Form validation                           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ˆ Request Flow Timeline

```
Time:    User Action â†’ Framework â†’ Processing â†’ Response
â”‚
â”œâ”€ T0:   Click "Upload Schema"
â”œâ”€ T0+:  Page loads (upload_schema.html)
â”‚
â”œâ”€ T1:   User selects PDF + clicks Submit
â”œâ”€ T1+:  POST /upload-schema
â”œâ”€ T1+:  Flask validates file
â”œâ”€ T1+:  Save file to disk
â”œâ”€ T1+:  Execute scheme_extractor.py (subprocess)
â”œâ”€ T1+:  scheme_extractor opens PDF
â”œâ”€ T1+:  PyMuPDF extracts/OCRs text
â”œâ”€ T1+:  Send to GPT-4 for structuring
â”œâ”€ T1+:  Parse JSON response
â”œâ”€ T1+:  Store in MongoDB (schema_db)
â”œâ”€ T2:   Return JSON response to Flask
â”œâ”€ T2+:  Flask returns success + Exam ID
â”œâ”€ T2+:  JavaScript displays result
â”œâ”€ T3:   User sees success message + Exam ID
â”‚
(User saves Exam ID)
â”‚
â”œâ”€ T4:   Click "Upload Script"
â”œâ”€ T4+:  Page loads (upload_script.html)
â”œâ”€ T5:   Enter Exam ID + Student ID + select PDF
â”œâ”€ T6:   POST /upload-script
â”œâ”€ T6+:  Flask validates
â”œâ”€ T6+:  Execute ocr_pdf.py
â”œâ”€ T6+:  For each page: OCR via OpenAI
â”œâ”€ T6+:  Store results in MongoDB (ai_evaluation_system)
â”œâ”€ T7:   Return success response
â”œâ”€ T8:   JavaScript displays success + links
â”‚
â”œâ”€ T9:   Click "Evaluate"
â”œâ”€ T9+:  Page loads (evaluate.html)
â”œâ”€ T10:  Click "Run Evaluation"
â”œâ”€ T11:  POST /evaluate
â”œâ”€ T11+: Execute comparator.py
â”œâ”€ T11+: Load schema from schema_db
â”œâ”€ T11+: Load OCR from ai_evaluation_system
â”œâ”€ T11+: Generate embeddings + calculate similarity
â”œâ”€ T11+: Call Gemini for verification
â”œâ”€ T11+: Store results in result_db
â”œâ”€ T12:  Return result ID
â”œâ”€ T13:  JavaScript redirects to Results page
â”‚
â”œâ”€ T14:  Results page loads
â”œâ”€ T14+: GET /results
â”œâ”€ T14+: Flask fetches from result_db
â”œâ”€ T14+: Render results.html with data
â”œâ”€ T15:  User sees scores and metrics
â”‚
â””â”€ T16+: User can: Manual Eval, View OCR, or other actions
```

## ğŸ¯ Error Handling Flow

```
Action â†’ Validation â†’ Processing â†’ Error Check â†’ Response

Valid Path:   âœ“ Input â†’ âœ“ File â†’ âœ“ Processing â†’ Success JSON
Invalid Path: âœ— Input â†’ Error Response (400)
File Path:    âœ“ Input â†’ âœ— File â†’ Error Response (400)
Server Path:  âœ“ Input â†’ âœ“ File â†’ âœ— Processing â†’ Error Response (500)

Errors Returned:
{
  "success": false,
  "message": "Error description"
}

HTTP Codes:
- 200: OK
- 400: Bad Request (validation, file errors)
- 404: Not Found
- 500: Server Error (processing, CLI, API)
```

---

**Architecture Version**: 1.0  
**Last Updated**: November 17, 2025
